"""
vLLM ê³ ì† íŒíŠ¸ ìƒì„± ì‹œìŠ¤í…œ - ë‹¨ìˆœí™” ë²„ì „
ë‹¨ì¼ vLLM ëª¨ë¸ë¡œ ë¹ ë¥¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ì— ì§‘ì¤‘
"""
import argparse
import gradio as gr
import json
import os
import time
from datetime import datetime
from typing import List, Dict, Optional
from pathlib import Path

from config import Config
from models.model_config import VLLM_MODELS
from models.model_inference import VLLMInference


class VLLMHintApp:
    """vLLM ì „ìš© íŒíŠ¸ ìƒì„± ì• í”Œë¦¬ì¼€ì´ì…˜"""

    def __init__(self, data_path: str, vllm_url: str = "http://localhost:8000/v1"):
        self.data_path = data_path
        self.problems = self.load_problems()
        self.vllm_url = vllm_url
        self.current_problem = None
        self.current_model = None

        # vLLM ì„œë²„ ì—°ê²° ì²´í¬
        self.check_vllm_connection()

    def check_vllm_connection(self):
        """vLLM ì„œë²„ ì—°ê²° í™•ì¸"""
        try:
            self.current_model = VLLMInference(
                model_name="vLLM-Server",
                base_url=self.vllm_url,
                timeout=60
            )
            print(f"âœ… vLLM ì„œë²„ ì—°ê²° ì„±ê³µ: {self.vllm_url}")
        except Exception as e:
            print(f"âš ï¸  vLLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            print(f"    {self.vllm_url}ì— vLLM ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
            self.current_model = None

    def load_problems(self) -> List[Dict]:
        """ë¬¸ì œ ë°ì´í„° ë¡œë“œ"""
        with open(self.data_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def get_problem_list(self) -> List[str]:
        """ë¬¸ì œ ëª©ë¡"""
        return [
            f"#{p['problem_id']} - {p['title']} (Level {p['level']})"
            for p in self.problems
        ]

    def load_problem(self, problem_selection: str):
        """ì„ íƒëœ ë¬¸ì œ ë¡œë“œ"""
        if not problem_selection:
            return "ë¬¸ì œë¥¼ ì„ íƒí•˜ì„¸ìš”.", ""

        try:
            problem_id = problem_selection.split('#')[1].split(' -')[0].strip()

            self.current_problem = None
            for p in self.problems:
                if str(p['problem_id']) == str(problem_id):
                    self.current_problem = p
                    break

            if not self.current_problem:
                return "âŒ ë¬¸ì œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", ""

            problem_md = self._format_problem_display()
            return problem_md, "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"

        except Exception as e:
            return f"âŒ ì˜¤ë¥˜: {str(e)}", ""

    def _format_problem_display(self) -> str:
        """ë¬¸ì œ í‘œì‹œ í¬ë§·"""
        p = self.current_problem
        md = f"""# {p['title']}

**ë‚œì´ë„:** Level {p['level']} | **íƒœê·¸:** {', '.join(p['tags'])}

---

## ğŸ“‹ ë¬¸ì œ ì„¤ëª…
{p['description']}

## ğŸ“¥ ì…ë ¥
{p['input_description']}

## ğŸ“¤ ì¶œë ¥
{p['output_description']}

## ğŸ’¡ ì˜ˆì œ
"""
        for i, example in enumerate(p['examples'], 1):
            input_txt = example.get('input', '') if example.get('input') else '(ì—†ìŒ)'
            output_txt = example.get('output', '') if example.get('output') else '(ì—†ìŒ)'
            md += f"\n**ì˜ˆì œ {i}**\n```\nì…ë ¥: {input_txt}\nì¶œë ¥: {output_txt}\n```\n"

        return md

    def generate_hint(self, user_code: str, temperature: float):
        """íŒíŠ¸ ìƒì„± (vLLM ì‚¬ìš©)"""
        if not self.current_problem:
            return "âŒ ë¨¼ì € ë¬¸ì œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.", ""

        if not user_code.strip():
            return "âŒ ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", ""

        if not self.current_model:
            return "âŒ vLLM ì„œë²„ì— ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”.", ""

        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._create_hint_prompt(user_code)

        # vLLMìœ¼ë¡œ íŒíŠ¸ ìƒì„± (ì‹œê°„ ì¸¡ì •)
        start_time = time.time()

        try:
            result = self.current_model.generate_hint(
                prompt=prompt,
                max_tokens=512,
                temperature=temperature
            )

            elapsed_time = time.time() - start_time

            if result.get('error'):
                return f"âŒ ìƒì„± ì‹¤íŒ¨: {result['error']}", ""

            hint = result.get('hint', '(ë¹ˆ ì‘ë‹µ)')

            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ í¬ë§·íŒ…
            metrics = f"""
## âš¡ ì¶”ë¡  ì„±ëŠ¥
- **ì†Œìš” ì‹œê°„:** {elapsed_time:.3f}ì´ˆ
- **Temperature:** {temperature}
- **Model:** {self.current_model.model_name}
"""

            return hint, metrics

        except Exception as e:
            return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", ""

    def _create_hint_prompt(self, user_code: str) -> str:
        """Socratic V6 í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        p = self.current_problem

        # ì²« ë²ˆì§¸ solution ì‚¬ìš©
        solutions = p.get('solutions', [])
        if not solutions:
            next_step = "ë¬¸ì œ í•´ê²°"
        else:
            solution = solutions[0]
            logic_steps = solution.get('logic_steps', [])
            if logic_steps:
                next_step = logic_steps[0].get('goal', 'ë¬¸ì œ í•´ê²°')
            else:
                next_step = "ë¬¸ì œ í•´ê²°"

        prompt = f"""ë‹¹ì‹ ì€ í•™ìƒì˜ í˜¸ê¸°ì‹¬ì„ ìê·¹í•˜ê³  ìŠ¤ìŠ¤ë¡œ ë°œê²¬í•˜ê²Œ ë§Œë“œëŠ” ì°½ì˜ì  ë©˜í† ì…ë‹ˆë‹¤.

### í•™ìƒì˜ í˜„ì¬ ì½”ë“œ:
```python
{user_code}
```

### í•µì‹¬ ë¯¸ì…˜:
í•™ìƒì´ ë‹¤ìŒ ë‹¨ê³„ì¸ "{next_step}"ì˜ í•„ìš”ì„±ì„ **ìŠ¤ìŠ¤ë¡œ ê¹¨ë‹«ê³  ì—´ë§í•˜ë„ë¡** ë§Œë“œì„¸ìš”.
ì§ì ‘ ë‹µì„ ì£¼ì§€ ë§ê³ , í•™ìƒì˜ ìƒìƒë ¥ê³¼ í˜¸ê¸°ì‹¬ì„ í­ë°œì‹œí‚¤ëŠ” ì§ˆë¬¸ì„ ë˜ì§€ì„¸ìš”.

### ë™ê¸° ìœ ë°œ ì „ëµ (ë°˜ë“œì‹œ ì ìš©):

1. **ê·œëª¨ í™•ì¥ ì‹œë‚˜ë¦¬ì˜¤**
   - ì§€ê¸ˆì€ ì‘ë™í•˜ì§€ë§Œ, ë°ì´í„°ê°€ 1000ë°° ëŠ˜ì–´ë‚˜ë©´?
   - ì‚¬ìš©ìê°€ 100ë§Œ ëª…ì´ ë˜ë©´?

2. **ì‹¤ìƒí™œ ì—°ê²°**
   - ìœ íŠœë¸ŒëŠ” ìˆ˜ë°±ë§Œ ì˜ìƒì„ ì–´ë–»ê²Œ ê´€ë¦¬í• ê¹Œ?
   - ê²Œì„ì—ì„œ ì•„ì´í…œì´ ìˆ˜ì²œ ê°œë©´ ì–´ë–»ê²Œ ì²˜ë¦¬í• ê¹Œ?

3. **ë¶ˆí¸í•¨ ìê·¹**
   - ê°™ì€ ì½”ë“œë¥¼ 100ë²ˆ ë³µì‚¬í•´ì•¼ í•œë‹¤ë©´?
   - ë§¤ë²ˆ ì†ìœ¼ë¡œ í•˜ë‚˜ì”© í™•ì¸í•´ì•¼ í•œë‹¤ë©´?

4. **í˜¸ê¸°ì‹¬ ìœ ë°œ**
   - ì™œ í”„ë¡œ ê°œë°œìë“¤ì€ í•­ìƒ ì´ íŒ¨í„´ì„ ì‚¬ìš©í• ê¹Œ?
   - ë” ë˜‘ë˜‘í•œ ë°©ë²•ì´ ìˆë‹¤ë©´ ì–´ë–»ê²Œ ë³´ì¼ê¹Œ?

5. **ì„±ì·¨ê° ì˜ˆê³ **
   - ì´ê²ƒë§Œ í•´ê²°í•˜ë©´ í›¨ì”¬ ê°•ë ¥í•´ì§ˆ í…ë°
   - í•œ ì¤„ë§Œ ë°”ê¾¸ë©´ ëª¨ë“  ê±¸ ìë™í™”í•  ìˆ˜ ìˆëŠ”ë°

### ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­:
âŒ í•¨ìˆ˜ëª…, ë³€ìˆ˜ëª…, ì½”ë“œ í‚¤ì›Œë“œ ì§ì ‘ ì–¸ê¸‰
âŒ "for ë°˜ë³µë¬¸", "if ì¡°ê±´ë¬¸" ê°™ì€ ê¸°ìˆ  ìš©ì–´
âŒ "~ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”", "~ë¥¼ ì¶”ê°€í•˜ì„¸ìš”" ê°™ì€ ì§ì ‘ ì§€ì‹œ
âŒ ì •ë‹µì˜ íŒíŠ¸ê°€ ë˜ëŠ” êµ¬ì²´ì  í‘œí˜„
âŒ ì˜ˆì‹œ ì½”ë“œ ì¡°ê°

### ì¶œë ¥ í˜•ì‹:
ë‹¨ 1ê°œì˜ ì§ˆë¬¸ë§Œ ì‘ì„±í•˜ì„¸ìš”. ì„¤ëª…, ë‹µë³€, ì¶”ê°€ íŒíŠ¸ ì¼ì²´ ê¸ˆì§€.
ì§ˆë¬¸ì€ 30-50ë‹¨ì–´ ì´ë‚´ë¡œ ê°„ê²°í•˜ë©´ì„œë„ ê°•ë ¬í•˜ê²Œ.

ì§ˆë¬¸:"""
        return prompt


def create_vllm_ui(app: VLLMHintApp):
    """vLLM ì „ìš© ë‹¨ìˆœí™” UI"""

    with gr.Blocks(title="vLLM ê³ ì† íŒíŠ¸ ìƒì„±", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# âš¡ vLLM ê³ ì† íŒíŠ¸ ìƒì„± ì‹œìŠ¤í…œ")
        gr.Markdown("vLLM ì„œë²„ë¥¼ í†µí•œ 15-24ë°° ë¹ ë¥¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸")

        # vLLM ì—°ê²° ìƒíƒœ
        if app.current_model:
            gr.Markdown(f"âœ… **vLLM ì„œë²„ ì—°ê²°ë¨:** `{app.vllm_url}`")
        else:
            gr.Markdown(f"âš ï¸ **vLLM ì„œë²„ ë¯¸ì—°ê²°:** `{app.vllm_url}` - ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”!")

        gr.Markdown("---")

        # ë¬¸ì œ ì„ íƒ
        with gr.Row():
            problem_dropdown = gr.Dropdown(
                choices=app.get_problem_list(),
                label="ğŸ“š ë¬¸ì œ ì„ íƒ",
                interactive=True,
                scale=3
            )
            load_btn = gr.Button("ğŸ“‚ ë¶ˆëŸ¬ì˜¤ê¸°", variant="primary", scale=1)

        problem_display = gr.Markdown("")

        gr.Markdown("---")

        # ì½”ë“œ ì…ë ¥
        gr.Markdown("## ğŸ’» ì½”ë“œ ì‘ì„±")
        user_code = gr.Code(
            label="Python ì½”ë“œ",
            language="python",
            lines=12,
            value="# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
        )

        # Temperature ì¡°ì ˆ
        gr.Markdown("### ğŸŒ¡ï¸ Temperature (ì°½ì˜ì„± ì¡°ì ˆ)")
        temperature_slider = gr.Slider(
            minimum=0.1,
            maximum=1.0,
            value=0.75,
            step=0.05,
            label="Temperature",
            info="ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì , ë†’ì„ìˆ˜ë¡ ì°½ì˜ì ",
            interactive=True
        )

        hint_btn = gr.Button("ğŸ’¡ íŒíŠ¸ ìƒì„± (vLLM)", variant="primary", size="lg")

        gr.Markdown("---")

        # íŒíŠ¸ ê²°ê³¼
        gr.Markdown("## ğŸ¯ ìƒì„±ëœ íŒíŠ¸")
        hint_output = gr.Markdown("_íŒíŠ¸ê°€ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤_")

        gr.Markdown("---")

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        gr.Markdown("## ğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­")
        metrics_output = gr.Markdown("_ì¶”ë¡  ì„±ëŠ¥ì´ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤_")

        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
        load_btn.click(
            fn=app.load_problem,
            inputs=[problem_dropdown],
            outputs=[problem_display, user_code]
        )

        hint_btn.click(
            fn=app.generate_hint,
            inputs=[user_code, temperature_slider],
            outputs=[hint_output, metrics_output]
        )

    return demo


if __name__ == "__main__":
    # ëª…ë ¹í–‰ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description="vLLM Hint Generation System")
    parser.add_argument("--server-name", type=str, default=None,
                       help="Server host (default: 127.0.0.1, use 0.0.0.0 for RunPod)")
    parser.add_argument("--server-port", type=int, default=7860,
                       help="Server port (default: 7860)")
    parser.add_argument("--share", action="store_true",
                       help="Create public share link")
    parser.add_argument("--no-browser", action="store_true",
                       help="Don't auto-open browser")
    parser.add_argument("--vllm-url", type=str, default=None,
                       help="vLLM server URL (default: from .env)")
    args = parser.parse_args()

    print("\n" + "âš¡" * 30)
    print("vLLM ê³ ì† íŒíŠ¸ ìƒì„± ì‹œìŠ¤í…œ")
    print("âš¡" * 30 + "\n")

    # RunPod í™˜ê²½ ìë™ ê°ì§€
    is_runpod = os.getenv("RUNPOD_POD_ID") is not None or os.getenv("PUBLIC_URL") is not None

    if is_runpod and args.server_name is None:
        args.server_name = "0.0.0.0"
        args.share = True
        args.no_browser = True
        print("ğŸš€ RunPod í™˜ê²½ ê°ì§€ë¨!")

    # vLLM URL ì„¤ì •
    vllm_url = args.vllm_url or Config.VLLM_SERVER_URL or "http://localhost:8000/v1"

    # ë°ì´í„° ê²½ë¡œ í™•ì¸
    DATA_PATH = Config.DATA_FILE_PATH
    if not DATA_PATH.exists():
        print(f"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DATA_PATH}")
        exit(1)

    # ì•± ì´ˆê¸°í™”
    print(f"ğŸ“š ë¬¸ì œ ë°ì´í„° ë¡œë”©: {DATA_PATH}")
    app = VLLMHintApp(str(DATA_PATH), vllm_url=vllm_url)
    print(f"âœ… {len(app.problems)}ê°œ ë¬¸ì œ ë¡œë“œ ì™„ë£Œ!\n")

    # UI ìƒì„± ë° ì‹¤í–‰
    print("ğŸŒ Gradio UI ì‹œì‘...\n")
    demo = create_vllm_ui(app)

    # Launch ì„¤ì •
    launch_kwargs = {
        "server_port": args.server_port,
        "share": args.share,
        "inbrowser": not args.no_browser
    }

    if args.server_name:
        launch_kwargs["server_name"] = args.server_name

    demo.launch(**launch_kwargs)
